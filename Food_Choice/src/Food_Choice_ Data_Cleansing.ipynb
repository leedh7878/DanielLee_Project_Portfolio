{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning & EDA Practice**\n",
    "### by Daniel Lee"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose is to clean the data, not to make statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* our data is stored in csvfile. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Description**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori = pd.read_csv(\"../data/Food_choices/food_coded.csv\", low_memory=False)\n",
    "data_cleaning = data_ori.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125 entries, 0 to 124\n",
      "Data columns (total 61 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   GPA                           123 non-null    object \n",
      " 1   Gender                        125 non-null    int64  \n",
      " 2   breakfast                     125 non-null    int64  \n",
      " 3   calories_chicken              125 non-null    int64  \n",
      " 4   calories_day                  106 non-null    float64\n",
      " 5   calories_scone                124 non-null    float64\n",
      " 6   coffee                        125 non-null    int64  \n",
      " 7   comfort_food                  124 non-null    object \n",
      " 8   comfort_food_reasons          123 non-null    object \n",
      " 9   comfort_food_reasons_coded    106 non-null    float64\n",
      " 10  cook                          122 non-null    float64\n",
      " 11  comfort_food_reasons_coded.1  125 non-null    int64  \n",
      " 12  cuisine                       108 non-null    float64\n",
      " 13  diet_current                  124 non-null    object \n",
      " 14  diet_current_coded            125 non-null    int64  \n",
      " 15  drink                         123 non-null    float64\n",
      " 16  eating_changes                122 non-null    object \n",
      " 17  eating_changes_coded          125 non-null    int64  \n",
      " 18  eating_changes_coded1         125 non-null    int64  \n",
      " 19  eating_out                    125 non-null    int64  \n",
      " 20  employment                    116 non-null    float64\n",
      " 21  ethnic_food                   125 non-null    int64  \n",
      " 22  exercise                      112 non-null    float64\n",
      " 23  father_education              124 non-null    float64\n",
      " 24  father_profession             122 non-null    object \n",
      " 25  fav_cuisine                   123 non-null    object \n",
      " 26  fav_cuisine_coded             125 non-null    int64  \n",
      " 27  fav_food                      123 non-null    float64\n",
      " 28  food_childhood                124 non-null    object \n",
      " 29  fries                         125 non-null    int64  \n",
      " 30  fruit_day                     125 non-null    int64  \n",
      " 31  grade_level                   125 non-null    int64  \n",
      " 32  greek_food                    125 non-null    int64  \n",
      " 33  healthy_feeling               125 non-null    int64  \n",
      " 34  healthy_meal                  124 non-null    object \n",
      " 35  ideal_diet                    124 non-null    object \n",
      " 36  ideal_diet_coded              125 non-null    int64  \n",
      " 37  income                        124 non-null    float64\n",
      " 38  indian_food                   125 non-null    int64  \n",
      " 39  italian_food                  125 non-null    int64  \n",
      " 40  life_rewarding                124 non-null    float64\n",
      " 41  marital_status                124 non-null    float64\n",
      " 42  meals_dinner_friend           122 non-null    object \n",
      " 43  mother_education              122 non-null    float64\n",
      " 44  mother_profession             123 non-null    object \n",
      " 45  nutritional_check             125 non-null    int64  \n",
      " 46  on_off_campus                 124 non-null    float64\n",
      " 47  parents_cook                  125 non-null    int64  \n",
      " 48  pay_meal_out                  125 non-null    int64  \n",
      " 49  persian_food                  124 non-null    float64\n",
      " 50  self_perception_weight        124 non-null    float64\n",
      " 51  soup                          124 non-null    float64\n",
      " 52  sports                        123 non-null    float64\n",
      " 53  thai_food                     125 non-null    int64  \n",
      " 54  tortilla_calories             124 non-null    float64\n",
      " 55  turkey_calories               125 non-null    int64  \n",
      " 56  type_sports                   99 non-null     object \n",
      " 57  veggies_day                   125 non-null    int64  \n",
      " 58  vitamins                      125 non-null    int64  \n",
      " 59  waffle_calories               125 non-null    int64  \n",
      " 60  weight                        123 non-null    object \n",
      "dtypes: float64(20), int64(27), object(14)\n",
      "memory usage: 59.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# info function: including the index dtype and columns, non-null values and memory usage.\n",
    "data_cleaning.info()\n",
    "# data_cleaning.drop_duplicates() # check duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Numeric columns:\n",
      "GPA\n",
      "comfort_food\n",
      "comfort_food_reasons\n",
      "diet_current\n",
      "eating_changes\n",
      "father_profession\n",
      "fav_cuisine\n",
      "food_childhood\n",
      "healthy_meal\n",
      "ideal_diet\n",
      "meals_dinner_friend\n",
      "mother_profession\n",
      "type_sports\n",
      "weight\n",
      "\n",
      "Numeric columns:\n",
      "Gender\n",
      "breakfast\n",
      "calories_chicken\n",
      "calories_day\n",
      "calories_scone\n",
      "coffee\n",
      "comfort_food_reasons_coded\n",
      "cook\n",
      "comfort_food_reasons_coded.1\n",
      "cuisine\n",
      "diet_current_coded\n",
      "drink\n",
      "eating_changes_coded\n",
      "eating_changes_coded1\n",
      "eating_out\n",
      "employment\n",
      "ethnic_food\n",
      "exercise\n",
      "father_education\n",
      "fav_cuisine_coded\n",
      "fav_food\n",
      "fries\n",
      "fruit_day\n",
      "grade_level\n",
      "greek_food\n",
      "healthy_feeling\n",
      "ideal_diet_coded\n",
      "income\n",
      "indian_food\n",
      "italian_food\n",
      "life_rewarding\n",
      "marital_status\n",
      "mother_education\n",
      "nutritional_check\n",
      "on_off_campus\n",
      "parents_cook\n",
      "pay_meal_out\n",
      "persian_food\n",
      "self_perception_weight\n",
      "soup\n",
      "sports\n",
      "thai_food\n",
      "tortilla_calories\n",
      "turkey_calories\n",
      "veggies_day\n",
      "vitamins\n",
      "waffle_calories\n"
     ]
    }
   ],
   "source": [
    "# pandas.Dataframe.select_dtypes() function: Return a subset of the DataFrame’s columns based on the column dtypes.\n",
    "\n",
    "obj_df = data_cleaning.select_dtypes(include=['object'])\n",
    "num_df = data_cleaning.select_dtypes(exclude=['object'])\n",
    "\n",
    "#helpfunction to seperate categorical and numerical features\n",
    "def printColumnTypes(non_numeric_df, numeric_df):\n",
    "    '''separates non-numeric and numeric columns'''\n",
    "    print(\"Non-Numeric columns:\")\n",
    "    for col in non_numeric_df:\n",
    "        print(f\"{col}\")\n",
    "    print(\"\")\n",
    "    print(\"Numeric columns:\")\n",
    "    for col in numeric_df:\n",
    "        print(f\"{col}\")\n",
    "\n",
    "printColumnTypes(obj_df, num_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a few problems for missing data method\n",
    "\n",
    "* For example, by dropping rows/columns, you’re essentially losing information that might be useful for prediction\n",
    "\n",
    "* On the other hand, imputing values will introduce bias to your data but it still might better than removing your features.\n",
    "\n",
    "Here is a great analogy for this dilemma in this article by Elite Data Science.\n",
    "\n",
    "Missing data is like missing a puzzle piece. If you drop it, that’s like pretending the puzzle slot isn’t there. If you impute it, that’s like trying to squeeze in a piece from somewhere else in the puzzle.\n",
    "\n",
    "source:https://medium.com/bitgrit-data-science-publication/data-cleaning-with-python-f6bc3da64e45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPA                  2\n",
      "Gender               0\n",
      "breakfast            0\n",
      "calories_chicken     0\n",
      "calories_day        19\n",
      "                    ..\n",
      "type_sports         26\n",
      "veggies_day          0\n",
      "vitamins             0\n",
      "waffle_calories      0\n",
      "weight               2\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_per_column = data_cleaning.isnull().sum()\n",
    "# .sum() funciton return series(dataframe with one column; have different parameter from dataframe)\n",
    "\n",
    "print(missing_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "shape function give give dimension of the dataframe which is [x,y]. length:x -> index = 0, width:y -> index = 1\n",
    "Thus, we can use that info to find number of data and attributes.\n",
    "'''\n",
    "num_obs = np.product(data_ori.shape[0])\n",
    "num_attr = np.product(data_ori.shape[1])\n",
    "# This approach may not needed if we use info() instead but, who know? there might be an update in data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Threshold for drop attributes would be 30% for big data and 20% for small data.\n",
    "'''\n",
    "# pandas.DataFrame.drop: Drop specified labels from rows or columns.\n",
    "\n",
    "# pandas.DataFrame.iloc = for index, .loc = for column name\n",
    "missing_percentage = (missing_per_column/num_obs)\n",
    "\n",
    "#lt = less than, gt = greater than, le = less and equal, ge = greater and equal\n",
    "\n",
    "feature_to_drop = missing_percentage[missing_percentage.ge(0.2)]\n",
    "feature_to_keep = missing_percentage[missing_percentage.lt(0.2)]\n",
    "#()parameter: subset of series, []parameter: index of subset\n",
    "\n",
    "feature_to_drop_index = feature_to_drop.index\n",
    "feature_to_keep_index = feature_to_keep.index\n",
    "\n",
    "data_cleaning = data_cleaning[feature_to_keep_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much simpler way to drop column with certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna(thresh): Require that many non-NA values.\n",
    "\n",
    "data_cleaning2 = data_ori.copy()\n",
    "thresh4data = len(data_cleaning2)*0.8\n",
    "data_cleaning2 = data_cleaning2.dropna(axis =1, thresh=thresh4data)\n",
    "# dropna() function returns a new DataFrame with missing values removed and does not modify the original DataFrame in place.\n",
    "data_cleaning2 = data_cleaning2.dropna(axis =0, thresh= data_cleaning2.shape[1]*0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the dtypes function to distinguish between categorical and numerical data, we may find that some features that appear to be numerical are instead assigned to the categorical type.\n",
    "We have observed that the features, GPA and Weight, have the potential to be represented as numerical data. However, the raw data in these features require cleaning to achieve this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPA</th>\n",
       "      <th>comfort_food</th>\n",
       "      <th>comfort_food_reasons</th>\n",
       "      <th>diet_current</th>\n",
       "      <th>eating_changes</th>\n",
       "      <th>father_profession</th>\n",
       "      <th>fav_cuisine</th>\n",
       "      <th>food_childhood</th>\n",
       "      <th>healthy_meal</th>\n",
       "      <th>ideal_diet</th>\n",
       "      <th>meals_dinner_friend</th>\n",
       "      <th>mother_profession</th>\n",
       "      <th>type_sports</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.4</td>\n",
       "      <td>none</td>\n",
       "      <td>we dont have comfort</td>\n",
       "      <td>eat good and exercise</td>\n",
       "      <td>eat faster</td>\n",
       "      <td>profesor</td>\n",
       "      <td>Arabic cuisine</td>\n",
       "      <td>rice  and chicken</td>\n",
       "      <td>looks not oily</td>\n",
       "      <td>being healthy</td>\n",
       "      <td>rice, chicken,  soup</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>car racing</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.654</td>\n",
       "      <td>chocolate, chips, ice cream</td>\n",
       "      <td>Stress, bored, anger</td>\n",
       "      <td>I eat about three times a day with some snacks...</td>\n",
       "      <td>I eat out more than usual.</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>Italian</td>\n",
       "      <td>chicken and biscuits, beef soup, baked beans</td>\n",
       "      <td>Grains, Veggies, (more of grains and veggies),...</td>\n",
       "      <td>Try to eat 5-6 small meals a day. While trying...</td>\n",
       "      <td>Pasta, steak, chicken</td>\n",
       "      <td>Nurse RN</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>frozen yogurt, pizza, fast food</td>\n",
       "      <td>stress, sadness</td>\n",
       "      <td>toast and fruit for breakfast, salad for lunch...</td>\n",
       "      <td>sometimes choosing to eat fast food instead of...</td>\n",
       "      <td>owns business</td>\n",
       "      <td>italian</td>\n",
       "      <td>mac and cheese, pizza, tacos</td>\n",
       "      <td>usually includes natural ingredients; nonproce...</td>\n",
       "      <td>i would say my ideal diet is my current diet</td>\n",
       "      <td>chicken and rice with veggies, pasta, some kin...</td>\n",
       "      <td>owns business</td>\n",
       "      <td>none</td>\n",
       "      <td>I'm not answering this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.2</td>\n",
       "      <td>Pizza, Mac and cheese, ice cream</td>\n",
       "      <td>Boredom</td>\n",
       "      <td>College diet, cheap and easy foods most nights...</td>\n",
       "      <td>Accepting cheap and premade/store bought foods</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Beef stroganoff, tacos, pizza</td>\n",
       "      <td>Fresh fruits&amp; vegetables, organic meats</td>\n",
       "      <td>Healthy, fresh veggies/fruits &amp; organic foods</td>\n",
       "      <td>Grilled chicken \\rStuffed Shells\\rHomemade Chili</td>\n",
       "      <td>Special Education Teacher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not sure, 240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5</td>\n",
       "      <td>Ice cream, chocolate, chips</td>\n",
       "      <td>Stress, boredom, cravings</td>\n",
       "      <td>I try to eat healthy but often struggle becaus...</td>\n",
       "      <td>I have eaten generally the same foods but I do...</td>\n",
       "      <td>IT</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Pasta, chicken tender, pizza</td>\n",
       "      <td>A lean protein such as grilled chicken, green ...</td>\n",
       "      <td>Ideally I would like to be able to eat healthi...</td>\n",
       "      <td>Chicken Parmesan, Pulled Pork, Spaghetti and m...</td>\n",
       "      <td>Substance Abuse Conselor</td>\n",
       "      <td>Softball</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GPA                      comfort_food        comfort_food_reasons   \n",
       "0    2.4                              none       we dont have comfort   \\\n",
       "1  3.654       chocolate, chips, ice cream        Stress, bored, anger   \n",
       "2    3.3   frozen yogurt, pizza, fast food             stress, sadness   \n",
       "3    3.2  Pizza, Mac and cheese, ice cream                     Boredom   \n",
       "4    3.5      Ice cream, chocolate, chips   Stress, boredom, cravings    \n",
       "\n",
       "                                        diet_current   \n",
       "0                              eat good and exercise  \\\n",
       "1  I eat about three times a day with some snacks...   \n",
       "2  toast and fruit for breakfast, salad for lunch...   \n",
       "3  College diet, cheap and easy foods most nights...   \n",
       "4  I try to eat healthy but often struggle becaus...   \n",
       "\n",
       "                                      eating_changes father_profession   \n",
       "0                                        eat faster          profesor   \\\n",
       "1                        I eat out more than usual.     Self employed    \n",
       "2  sometimes choosing to eat fast food instead of...     owns business   \n",
       "3     Accepting cheap and premade/store bought foods         Mechanic    \n",
       "4  I have eaten generally the same foods but I do...                IT   \n",
       "\n",
       "      fav_cuisine                                food_childhood   \n",
       "0  Arabic cuisine                            rice  and chicken   \\\n",
       "1         Italian  chicken and biscuits, beef soup, baked beans   \n",
       "2         italian                  mac and cheese, pizza, tacos   \n",
       "3        Turkish                  Beef stroganoff, tacos, pizza   \n",
       "4        Italian                  Pasta, chicken tender, pizza    \n",
       "\n",
       "                                        healthy_meal   \n",
       "0                                    looks not oily   \\\n",
       "1  Grains, Veggies, (more of grains and veggies),...   \n",
       "2  usually includes natural ingredients; nonproce...   \n",
       "3           Fresh fruits& vegetables, organic meats    \n",
       "4  A lean protein such as grilled chicken, green ...   \n",
       "\n",
       "                                          ideal_diet   \n",
       "0                                     being healthy   \\\n",
       "1  Try to eat 5-6 small meals a day. While trying...   \n",
       "2       i would say my ideal diet is my current diet   \n",
       "3     Healthy, fresh veggies/fruits & organic foods    \n",
       "4  Ideally I would like to be able to eat healthi...   \n",
       "\n",
       "                                 meals_dinner_friend   \n",
       "0                               rice, chicken,  soup  \\\n",
       "1                             Pasta, steak, chicken    \n",
       "2  chicken and rice with veggies, pasta, some kin...   \n",
       "3   Grilled chicken \\rStuffed Shells\\rHomemade Chili   \n",
       "4  Chicken Parmesan, Pulled Pork, Spaghetti and m...   \n",
       "\n",
       "           mother_profession  type_sports                    weight  \n",
       "0                 unemployed   car racing                       187  \n",
       "1                  Nurse RN   Basketball                        155  \n",
       "2              owns business         none  I'm not answering this.   \n",
       "3  Special Education Teacher          NaN             Not sure, 240  \n",
       "4   Substance Abuse Conselor     Softball                       190  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify non-numerical data in a column, we can look for the unique values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.4' '3.654' '3.3' '3.2' '3.5' '2.25' '3.8' '3.904' '3.4' '3.6' '3.1'\n",
      " nan '4' '2.2' '3.87' '3.7' '3.9' '2.8' '3' '3.65' '3.89' '2.9' '3.605'\n",
      " '3.83' '3.292' '3.35' 'Personal ' '2.6' '3.67' '3.73' '3.79 bitch' '2.71'\n",
      " '3.68' '3.75' '3.92' 'Unknown' '3.77' '3.63' '3.882']\n",
      "['187' '155' \"I'm not answering this. \" 'Not sure, 240' '190' '180' '137'\n",
      " '125' '116' '110' '264' '123' '185' '145' '170' '135' '165' '175' '195'\n",
      " '105' '160' '167' '115' '205' nan '128' '150' '140' '120' '100' '113'\n",
      " '168' '169' '200' '265' '192' '118' '210' '112' '144 lbs' '130' '127'\n",
      " '129' '260' '184' '230' '138' '156']\n"
     ]
    }
   ],
   "source": [
    "print(obj_df['GPA'].unique())\n",
    "print(obj_df['weight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The drop() method will remove an entire row from a DataFrame when given a single index label, instead of removing a single value from a specific column.\n",
    "\n",
    "# replace with correct format of data\n",
    "data_cleaning['GPA'] = data_cleaning['GPA'].replace('3.79 bitch', 3.79)\n",
    "\n",
    "# convert data into numeric, errors= 'coerce' change non-numeric to nan\n",
    "data_cleaning['GPA'] = pd.to_numeric(data_cleaning['GPA'], errors ='coerce')\n",
    "data_cleaning['weight'] = pd.to_numeric(data_cleaning['weight'],errors='coerce' )\n",
    "\n",
    "obj_df = obj_df.drop(columns=['GPA', 'weight'],axis=1)\n",
    "num_df = data_cleaning.select_dtypes(exclude=['object'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "object: texts, text values, or a mix of numeric and non-numeric values\n",
    "\n",
    "For the column with object datatype can be change to categorical data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       comfort_food        comfort_food_reasons   \n",
      "0                              none       we dont have comfort   \\\n",
      "1       chocolate, chips, ice cream        stress, bored, anger   \n",
      "2   frozen yogurt, pizza, fast food             stress, sadness   \n",
      "3  pizza, mac and cheese, ice cream                     boredom   \n",
      "4      ice cream, chocolate, chips   stress, boredom, cravings    \n",
      "\n",
      "                                        diet_current   \n",
      "0                              eat good and exercise  \\\n",
      "1  i eat about three times a day with some snacks...   \n",
      "2  toast and fruit for breakfast, salad for lunch...   \n",
      "3  college diet, cheap and easy foods most nights...   \n",
      "4  i try to eat healthy but often struggle becaus...   \n",
      "\n",
      "                                      eating_changes father_profession   \n",
      "0                                        eat faster          profesor   \\\n",
      "1                        i eat out more than usual.     self employed    \n",
      "2  sometimes choosing to eat fast food instead of...     owns business   \n",
      "3     accepting cheap and premade/store bought foods         mechanic    \n",
      "4  i have eaten generally the same foods but i do...                it   \n",
      "\n",
      "      fav_cuisine                                food_childhood   \n",
      "0  arabic cuisine                            rice  and chicken   \\\n",
      "1         italian  chicken and biscuits, beef soup, baked beans   \n",
      "2         italian                  mac and cheese, pizza, tacos   \n",
      "3        turkish                  beef stroganoff, tacos, pizza   \n",
      "4        italian                  pasta, chicken tender, pizza    \n",
      "\n",
      "                                        healthy_meal   \n",
      "0                                    looks not oily   \\\n",
      "1  grains, veggies, (more of grains and veggies),...   \n",
      "2  usually includes natural ingredients; nonproce...   \n",
      "3           fresh fruits& vegetables, organic meats    \n",
      "4  a lean protein such as grilled chicken, green ...   \n",
      "\n",
      "                                          ideal_diet   \n",
      "0                                     being healthy   \\\n",
      "1  try to eat 5-6 small meals a day. while trying...   \n",
      "2       i would say my ideal diet is my current diet   \n",
      "3     healthy, fresh veggies/fruits & organic foods    \n",
      "4  ideally i would like to be able to eat healthi...   \n",
      "\n",
      "                                 meals_dinner_friend   \n",
      "0                               rice, chicken,  soup  \\\n",
      "1                             pasta, steak, chicken    \n",
      "2  chicken and rice with veggies, pasta, some kin...   \n",
      "3   grilled chicken \\rstuffed shells\\rhomemade chili   \n",
      "4  chicken parmesan, pulled pork, spaghetti and m...   \n",
      "\n",
      "           mother_profession  \n",
      "0                 unemployed  \n",
      "1                  nurse rn   \n",
      "2              owns business  \n",
      "3  special education teacher  \n",
      "4   substance abuse conselor  \n"
     ]
    }
   ],
   "source": [
    "obj_columns = data_cleaning.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# use lambda function tp convert it to lowercase and fix grammar \n",
    "data_cleaning[obj_columns] = data_cleaning[obj_columns].apply(lambda x: (x.str.lower()))\n",
    "\n",
    "    \n",
    "print(data_cleaning[obj_columns].head())\n",
    "# print(data_cleaning['fav_cuisine'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the codebook, there are comments that some features are ideal for perform NLP(Nautral Language Processing). Thus, I used 3 features: comfort_food, comfort_food_reasons and diet_current\n",
    "\n",
    "I used NLTK python library\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'packages to download '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''packages to download '''\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ars.usda.gov/ARSUserFiles/80400530/pdf/1112/food_category_list.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract the data that we are going to use NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nlp = data_cleaning[['comfort_food', 'comfort_food_reasons', 'diet_current']]\n",
    "\n",
    "data_nlp_prev = data_nlp ## copies of orginal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is main function that chunk the sentence into word with POS tag using parser. It will also create tree that shows structure of sentence'''\n",
    "def chunk_NP(text, origin=True):    \n",
    "    grammar =  r'''\n",
    "    NP: {<RB>?<NN.*>+|<VBZ><NN>|<NN><CC><NN>|<DT>?<NN|NNS>+}\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    The rule states that whenever the chunk finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN) then the Noun Phrase(NP) chunk should be formed.\n",
    "    '''\n",
    "    \n",
    "    if type(text) is not str:\n",
    "        return ['none']\n",
    "    else:    \n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lotr_pos_tags = nltk.pos_tag(tokens)\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        tree = chunk_parser.parse(lotr_pos_tags)\n",
    "\n",
    "        noun_phrases = []\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() =='NP':\n",
    "                np_parts = []\n",
    "                for leaf in subtree.leaves():\n",
    "                    np_parts.append(leaf[0])\n",
    "                noun_phrases.append(\" \".join(np_parts))\n",
    "        return noun_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tree(text):\n",
    "    grammar =  r'''\n",
    "    NP: {<RB>?<NN.*>+|<VBZ><NN|NNS>|<NN|NNS><CC><NN|NNS>|<DT>?<NN|NNS>+}\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''RB to extract 'and'\n",
    "    '''\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lotr_pos_tags = nltk.pos_tag(tokens)\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(lotr_pos_tags)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pasta dishes', 'squash', 'food', 'pretzals', 'cups', 'candy', 'chicken wings', 'grandma homemade chocolate cake anything homemade', 'salty snacks', 'rice', 'toast', 'fritos', 'peanut butter', 'nuggets', 'noodle', 'ice-cream', 'any kinds', 'soup', 'chocolate brownie', 'chicken', 'moes', 'chips', 'doughnuts', 'cornbread', 'kit kat', 'popcorn', 'twizzlers', 'fast food', 'wings', 'cookie dough', 'ice crea', 'pot pie', 'chips sweets', 'pretzels', 'almonds', 'coffee', 'chicken curry', 'chocolate', 'grapes', 'reese', 'tikka masala', 'egg', 'chicken nuggs', 'home', 'salt', 'candy pop chocolate chipotle moe', 'milkshakes', 'fruit snacks', 'mix', 'pasta', 'grandma', 'watermelon', 'spaghetti', 'protein bars', 'cake', 'ranch', 'burgers', 'dr. pepper', 'cheesecake', 'omelet', 'none', 'beef jerky', 'mozzarella sticks', 'foods', 'pierogies', 'dip', 'macaroons', 'doritos', 'potatoes', 'icecream', 'deli sandwhich', 'snacks', 'cheese', 'potato', 'tuna sandwich', 'peppers', 'butter', 'cheez-its', 'cheeseburgers', 'ice cream', 'peanut butter sandwich', 'sweets', 'lasagne', 'soda', 'cookies', 'hamburgers', 'cereal', 'chilli', 'vinegar chips', 'sushi', 'brownies', 'banana sandwich', 'potato chips', 'chocolates', 'ritz', 'pizza cookies', 'pizza chocolate chips bagels ice capps', 'quinoa', 'macaroni', 'ice cream/milkshake', 'sponge candy', 'fires', 'slim jims', 'chicken fingers', 'broccoli', 'lasagna', 'pop', 'yogurt', 'cottage cheese', 'pizza', 'salsa', 'bread', 'dessets', 'fries', 'carrots', 'truffles', 'butter naan', 'mac', 'debbie snacks', 'burger', 'candy bars', 'pizza / wings / cheesecake', 'subs', 'burritos', 'meatball sub', 'pepsi', 'chocolate ice cream', 'chocolate bar', 'bread/crackers', 'donuts', 'chex-mix', 'nutella', 'potato soup', 'pancakes', 'fruit', 'mcdonalds', 'tomato soup', 'wine'}\n"
     ]
    }
   ],
   "source": [
    "new_column = []\n",
    "new_column2 = []\n",
    "\n",
    "for x in range(len(data_nlp[\"comfort_food\"])):\n",
    "\n",
    "    a = chunk_NP(data_nlp.loc[x,'comfort_food'])\n",
    "    new_column.append(a)\n",
    "\n",
    "    b = chunk_NP(data_nlp.loc[x, 'comfort_food_reasons'])\n",
    "    new_column2.append(b)\n",
    "\n",
    "flat_list = [item for sublist in new_column for item in sublist]\n",
    "\n",
    "\n",
    "\n",
    "print(set(flat_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the results from the unique value from comfort_food:\n",
    "\n",
    "{'doughnuts', 'pot pie', 'salt', 'carrots', 'mix', 'pop', 'fruit snacks', 'fast food', 'coffee', 'grandma', 'omelet', 'butter naan', 'chicken', 'icecream', 'ranch', 'meatball sub', 'soda', 'cereal', 'beef jerky', 'potato', 'brownies', 'tomato soup', 'candy pop chocolate chipotle moe', 'pizza chocolate chips bagels ice capps', 'potato chips', 'ice-cream', 'sushi', 'burritos', 'doritos', 'reese', 'squash', 'bread', 'pizza', 'dr. pepper', 'twizzlers', 'burgers', 'yogurt', 'ice crea', 'broccoli', 'any kinds', 'peanut butter', 'chocolates', 'mozzarella sticks', 'sponge candy', 'cheesecake', 'cheez-its', 'dessets', 'chilli', 'ice cream/milkshake', 'grandma homemade chocolate cake anything homemade', 'snacks', 'fruit', 'moes', 'mcdonalds', 'vinegar chips', 'wine', 'salty snacks', 'fritos', 'deli sandwhich', 'fries', 'pepsi', 'cheese', 'potato soup', 'donuts', 'chicken curry', 'wings', 'burger', 'cookies', 'truffles', 'chocolate brownie', 'banana sandwich', 'chicken fingers', 'chips sweets', 'pierogies', 'nuggets', 'potatoes', 'cookie dough', 'subs', 'cake', 'pasta dishes', 'slim jims', 'protein bars', 'sweets', 'soup', 'chex-mix', 'peppers', 'bread/crackers', 'dip', 'quinoa', 'lasagna', 'chips', 'tikka masala', 'pizza / wings / cheesecake', 'toast', 'pretzals', 'egg', 'popcorn', 'peanut butter sandwich', 'salsa', 'pasta', 'chocolate', 'candy', 'cheeseburgers', 'tuna sandwich', 'grapes', 'home', 'foods', 'macaroni', 'cornbread', 'chicken wings', 'chocolate ice cream', 'chocolate bar', 'watermelon', 'pancakes', 'lasagne', 'almonds', 'cottage cheese', 'milkshakes', 'chicken nuggs', 'pretzels', 'ritz', 'macaroons', 'noodle', 'nutella', 'cups', 'fires', 'rice', 'none', 'mac', 'debbie snacks', 'ice cream', 'candy bars', 'spaghetti', 'food', 'pizza cookies', 'butter', 'kit kat', 'hamburgers'}\n",
    "\n",
    "\n",
    "#### I manually removed certain words from the list that were not appropriate for describing food.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bread/crackers', 'icecream', 'pop', 'food', 'ice-cream', 'cups', 'home', 'salt', 'ranch', 'any kinds', 'candy pop chocolate chipotle moe', 'pizza chocolate chips bagels ice capps', 'ice cream/milkshake', 'grandma homemade chocolate cake anything homemade', 'ice crea', 'mac'}\n"
     ]
    }
   ],
   "source": [
    "list_keep_1st = {'doughnuts', 'pot pie', 'carrots', 'mix', 'fruit snacks', 'fast food', 'coffee', 'grandma', 'omelet', 'butter naan', 'chicken', 'meatball sub', 'soda', 'cereal', 'beef jerky', 'potato', 'brownies', 'tomato soup', 'potato chips', 'sushi', 'burritos', 'doritos', 'reese', 'squash', 'bread', 'pizza', 'dr. pepper', 'twizzlers', 'burgers', 'yogurt', 'broccoli', 'peanut butter', 'chocolates', 'mozzarella sticks', 'sponge candy', 'cheesecake', 'cheez-its', 'dessets', 'chilli', 'snacks', 'fruit', 'moes', 'mcdonalds', 'vinegar chips', 'wine', 'salty snacks', 'fritos', 'deli sandwhich', 'fries', 'pepsi', 'cheese', 'potato soup', 'donuts', 'chicken curry', 'wings', 'burger', 'cookies', 'truffles', 'chocolate brownie', 'banana sandwich', 'chicken fingers', 'chips sweets', 'pierogies', 'nuggets', 'potatoes', 'cookie dough', 'subs', 'cake', 'pasta dishes', 'slim jims', 'protein bars', 'sweets', 'soup', 'chex-mix', 'peppers', 'dip', 'quinoa', 'lasagna', 'chips', 'tikka masala', 'pizza / wings / cheesecake', 'toast', 'pretzals', 'egg', 'popcorn', 'peanut butter sandwich', 'salsa', 'pasta', 'chocolate', 'candy', 'cheeseburgers', 'tuna sandwich', 'grapes','foods', 'macaroni', 'cornbread', 'chicken wings', 'chocolate ice cream', 'chocolate bar', 'watermelon', 'pancakes', 'lasagne', 'almonds', 'cottage cheese', 'milkshakes', 'chicken nuggs', 'pretzels', 'ritz', 'macaroons', 'noodle', 'nutella', 'fires', 'rice', 'none', 'debbie snacks', 'ice cream', 'candy bars', 'spaghetti', 'pizza cookies', 'butter', 'kit kat', 'hamburgers'}\n",
    "\n",
    "list_removal_1st = set(flat_list)-list_keep_1st\n",
    "print(list_removal_1st) ##TODO Check what words werer sorted out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_with_words(sentences, words):\n",
    "    found_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split()\n",
    "        for word in words:\n",
    "            if word.lower() in [s.lower() for s in sentence_words]:\n",
    "                found_sentences.append(sentence)\n",
    "                break\n",
    "    return found_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'float'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for x in data_nlp['comfort_food']:\n",
    "    print(x if type(x)== float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# data_nlp.loc[:,'comfort_food'] = data_nlp['comfort_food'].astype(str)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m incorrect_food_name \u001b[39m=\u001b[39m find_sentences_with_words(data_nlp[\u001b[39m\"\u001b[39;49m\u001b[39mcomfort_food\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(), list_removal_1st)\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m incorrect_food_name: \n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(x)\n",
      "Cell \u001b[1;32mIn[100], line 4\u001b[0m, in \u001b[0;36mfind_sentences_with_words\u001b[1;34m(sentences, words)\u001b[0m\n\u001b[0;32m      2\u001b[0m found_sentences \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m----> 4\u001b[0m     sentence_words \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39;49msplit()\n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[0;32m      6\u001b[0m         \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m [s\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sentence_words]:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# data_nlp.loc[:,'comfort_food'] = data_nlp['comfort_food'].astype(str)\n",
    "\n",
    "incorrect_food_name = find_sentences_with_words(data_nlp[\"comfort_food\"].tolist(), list_removal_1st)\n",
    "\n",
    "for x in incorrect_food_name: \n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chinese food, korean food \n",
    "mac and cheese vs mac n cheese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = \"ice cream, pizza, chinese food\".lower()\n",
    "string1 = \"tomato soup, pizza, fritos, meatball sub, dr. pepper\".lower()\n",
    "string3 = \"chex mix,wegmans\"\n",
    "get_Tree(string3).draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tree(text):\n",
    "    grammar =  r'''\n",
    "    NP: {<FT><NN>|<RB>?<NN.*>+|<NN><CC><NN>|<DT>?<NN|NNS>+}\n",
    "    '''\n",
    "\n",
    "    custom_tags_foodtypes = ['chinese', 'korean']\n",
    "    custom_tags_waste = ['grandma', 'homemade']\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    ## FT stand for food type\n",
    "    lotr_pos_tags = [\n",
    "        (word, 'FT') if word in custom_tags_foodtypes\n",
    "    else (word, tag) for word, tag in nltk.pos_tag(tokens)]\n",
    "                 \n",
    "    # lotr_pos_tags = nltk.pos_tag(tokens) ## previous version\n",
    "\n",
    "\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = chunk_parser.parse(lotr_pos_tags)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = \"pasta, grandma homemade chocolate cake anything homemade \".lower()\n",
    "string1 = \"frozen yogurt, pizza, fast food\".lower()\n",
    "get_Tree(string2).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in c:\n",
    "    print(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is an additional function I developed to sanitize informal language. Given my expertise, I couldn't come up with a more suitable alternative.'''\n",
    "\n",
    "def informal2formal(text):\n",
    "\n",
    "    text = remove_nonaplpha(text)\n",
    "    formal_tokens = []\n",
    "    informal_to_formal = {\n",
    "\n",
    "        \"n\" : \"and\",\n",
    "        \"crea\" : \"cream\"\n",
    " \n",
    "    }\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if informal_to_formal.get(token) is not None:\n",
    "            formal_tokens.append(informal_to_formal.get(token))\n",
    "        else:\n",
    "            formal_tokens.append(token)\n",
    "    \n",
    "    ## convert plural to singular\n",
    "    formal_tokens = [lemmatizer.lemmatize(word)for word in formal_tokens]\n",
    "\n",
    "    return formal_tokens\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_nonaplpha(text):\n",
    "    # Remove any non-alphabetic characters except for hyphens and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\,\\/-]', ' ', text)\n",
    "    # Replace multiple spaces or hyphens with a single space\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    # Replace hyphens with a space\n",
    "    text = text.replace('-', ' ')\n",
    "    ## Replace foward slash with a comma\n",
    "    text = text.replace('/', ', ')\n",
    "    # Trim leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is main function that chunk the sentence into word with POS tag using parser. It will also create tree that shows structure of sentence '''\n",
    "def chunk_NP(text, origin = True):\n",
    "    grammar =  r'''\n",
    "    NP: { <RB>?<NN>*|<NN><CC><NN>|<DT>?<JJ>*<NN|NNS>+|}\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    The rule states that whenever the chunk finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN) then the Noun Phrase(NP) chunk should be formed.\n",
    "    '''\n",
    "\n",
    "\n",
    "    if type(text) is not str:\n",
    "        return ['none']\n",
    "    else:    \n",
    "        if origin:\n",
    "            text = informal2formal(text)\n",
    "            \n",
    "        lotr_pos_tags = nltk.pos_tag(text)\n",
    "        chunk_parser = nltk.RegexpParser(grammar)\n",
    "        tree = chunk_parser.parse(lotr_pos_tags)\n",
    "\n",
    "        noun_phrases = []\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() =='NP':\n",
    "                np_parts = []\n",
    "                for leaf in subtree.leaves():\n",
    "                    np_parts.append(leaf[0])\n",
    "                noun_phrases.append(\" \".join(np_parts))\n",
    "        return noun_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tree(text):\n",
    "    grammar =  r'''\n",
    "    NP: { <RB>?<NN>*|<NN><CC><NN>|<DT>?<JJ>*<NN|NNS>+|}\n",
    "    '''\n",
    "    \n",
    "    # grammar = r'NP:{<NN><CC><NN> }'\n",
    "    '''RB to extract 'and'\n",
    "    '''\n",
    "\n",
    "    words_in_text = informal2formal(text)\n",
    "    lotr_pos_tags = nltk.pos_tag(words_in_text)\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "    tree = nltk.ne_chunk(lotr_pos_tags)\n",
    "\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(len(data_nlp[\"comfort_food\"])):\n",
    "\n",
    "    a = chunk_NP(data_nlp.loc[x,'comfort_food'])\n",
    "    data_nlp.at[x,\"comfort_food\"] = a\n",
    "\n",
    "    b = chunk_NP(data_nlp.loc[x, 'comfort_food_reasons'])\n",
    "    data_nlp.at[x,'comfort_food_reasons'] = b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.loc[:,['comfort_food','comfort_food_reasons', 'diet_current']] = data_nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data_nlp_prev['comfort_food']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_explode = data_cleaning.explode('comfort_food') \n",
    "## The explode() function is used to transform each element of a list-like to a row, replicating the index values.\n",
    "\n",
    "print(cf_explode['comfort_food'].unique())\n",
    "print(cf_explode['comfort_food'].unique().shape)\n",
    "print(cf_explode['comfort_food'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grandma homemade chocolate cake anything homemade\n",
    "ice crea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Pizza / Wings / Cheesecake\".lower()\n",
    "\n",
    "\n",
    "string2 = \"grandma homemade chocolate cake anything homemade\".lower()\n",
    "\n",
    "string3 = \"Saltfish, Candy and Kit Kat \"\n",
    "\n",
    "string4 = \"chocolate,ice cream/milkshake,cooky\"\n",
    "# print(chunk_NP(string2))\n",
    "get_Tree(string2).draw()\n",
    "# print(nltk.word_tokenize(string2))\n",
    "print(informal2formal(string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"ice cream/milkshake\"\n",
    "corrected_text = spell(text)\n",
    "\n",
    "print(corrected_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
